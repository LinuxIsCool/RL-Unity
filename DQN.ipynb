{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train an RL agent using DQN architecture in a Unity environment (bananaModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, defaultdict\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from dqnetwork import DQNetwork\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action space:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "### State space:\n",
    "- `37` - dimensions.\n",
    "- some samples include the agent's velocity.\n",
    "- ray-based perception in the forward direction of the agent.\n",
    "\n",
    "### Reward:\n",
    "\n",
    "- `+1` - Yellow Banana collected.\n",
    "- `-1` - Blue Banana collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "i = 0\n",
    "while True:\n",
    "    i+=1\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if(reward != 0):\n",
    "        print(reward)\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"iterations:\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (37x128) -> (128x64) -> (64x32) -> (32x4)\n",
    "input_features = [state_size, 128, 64, 32]\n",
    "output_features = [128, 64, 32, action_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQNetwork(input_features, output_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### define the agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import deque, defaultdict\n",
    "import torch.optim as optim\n",
    "from dqnetwork import DQNetwork\n",
    "import random\n",
    "from buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Defines the agent class for DQN using Double Q-learning and Prioritized Experience Replay architecture\"\"\"\n",
    "    def __init__(self, state_size=37, action_size=4, gamma=0.99, lr=0.001, update_every=5):\n",
    "        \"\"\"\n",
    "        Initializes the model.\n",
    "        ----\n",
    "        @param:\n",
    "        1. state_size: size of input # of states.\n",
    "        2. action_size: size of # of actions.\n",
    "        3. gamma: discounted return rate.\n",
    "        4. lr: learning rate for the model.\n",
    "        5. update_every: update target_model every X time-steps.\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.input_features = [state_size, 128, 64, 32]\n",
    "        self.output_features = [128, 64, 32, action_size]\n",
    "        \n",
    "        #Q-network : defines the 2 DQN (using doubling Q-learning architecture via fixed Q target)\n",
    "        self.qnetwork_local = DQNetwork(self.input_features, self.output_features)\n",
    "        self.qnetwork_target = DQNetwork(self.input_features, self.output_features)\n",
    "        #define the optimizer\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
    "        \n",
    "        #replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "        self.update_every = update_every\n",
    "        self.target_update_counter = 0\n",
    "        \n",
    "    def step(self, transition):\n",
    "        \"\"\"Performs forward pass of the tranisition\n",
    "        @param:\n",
    "        1. transition : (tuple) state, action, reward, next_state, done\n",
    "        \"\"\"\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(transition)\n",
    "        self.target_update_counter = (self.target_update_counter + 1) % self.update_every\n",
    "        \n",
    "        #Update target network to local network\n",
    "        if(self.target_update_counter == 0):\n",
    "            #primary condition to check if len(buffer) > batch_size\n",
    "            experiences = self.memory.sample()\n",
    "            self.train(experiences, self.gamma)\n",
    "    \n",
    "    def get_action(self, state, eps=0.0):\n",
    "        \"\"\"\n",
    "        Determines the action to perform based on epsilon-greedy method\n",
    "        @param:\n",
    "        1. state - list of current observations to determine an action for\n",
    "        2. eps - value for epsilon, stochastic measure.\n",
    "        @return:\n",
    "        - action = action chosen by either equiprobably Ï€ or using Q-table\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_val = self.qnetwork_local(state)\n",
    "        \n",
    "        self.qnetwork_local.train()#train local network\n",
    "        \n",
    "        #Epsilon-greedy selection\n",
    "        if(random.random() > eps):#exploit\n",
    "            return np.argmax(action_val.cpu().data.numpy())\n",
    "        \n",
    "        return random.choice(np.arange(self.action_size))#explore\n",
    "    \n",
    "    def train(self, experiences):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        @param:\n",
    "        1. experiences: (Tuple[torch.Variable]) (s,a,r,s',done)\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, done = experiences\n",
    "        \n",
    "        #Implement SGD using Adam as regularizer\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - done))\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        #set loss as mse.\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        #Update target network using soft update\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target)\n",
    "    \n",
    "    def soft_update(self, local_model, target_model):\n",
    "        \"\"\"\n",
    "        Update target network to local network using a soft update param, Ï„.\n",
    "        Î¸_target = Ï„*Î¸_local + (1 - Ï„)*Î¸_target\n",
    "        ------\n",
    "        @param:\n",
    "        1. local_model: (DQNetwork) local network model (weights will be copied from)\n",
    "        2. target_model: (DQNetwork) target network model (weights will be copied into)\n",
    "        \"\"\"\n",
    "        \n",
    "        target_model = TAU*local_model + (1 - TAU) * target_model\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the replay buffer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
