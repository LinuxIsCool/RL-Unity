{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train an RL agent using DQN architecture in a Unity environment (bananaModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, defaultdict\n",
    "import time\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Banana.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action space:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "### State space:\n",
    "- `37` - dimensions.\n",
    "- some samples include the agent's velocity.\n",
    "- ray-based perception in the forward direction of the agent.\n",
    "\n",
    "### Reward:\n",
    "\n",
    "- `+1` - Yellow Banana collected.\n",
    "- `-1` - Blue Banana collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [1.         0.         0.         0.         0.71256876 0.\n",
      " 0.         1.         0.         0.73509979 0.         0.\n",
      " 0.         1.         0.         1.         0.         0.\n",
      " 0.         0.31326672 0.         0.         1.         0.\n",
      " 0.58369923 0.         0.         0.         1.         0.\n",
      " 1.         0.         0.         0.         0.23518428 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "i = 0\n",
    "while True:\n",
    "    i+=1\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if(reward != 0):\n",
    "        print(reward)\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations: 300\n"
     ]
    }
   ],
   "source": [
    "print(\"iterations:\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (37x128) -> (128x64) -> (64x32) -> (32x4)\n",
    "input_features = [state_size, 128, 64, 32]\n",
    "output_features = [128, 64, 32, action_size]\n",
    "state = torch.tensor(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the feed forward NN used for the DQN Agent.\n",
    "    inherits nn.modules class.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_features, output_features, dropout_layers=[0.3, 0.1]):\n",
    "        \"\"\"\n",
    "        Initializes the model.\n",
    "        ------\n",
    "        @Param:\n",
    "        1. input_features: list of input dimensions for the NN.\n",
    "        2. output_features: list of corresponding output dimensions.\n",
    "        3. dropout_layers: list of dropout layers; keep_probs value (stochastic) of length < num_layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.state_size = input_features[0]#size of observational space\n",
    "        self.action_size = output_features[-1] #size of action space\n",
    "        self.FC = []#initialize list of FC layers\n",
    "        self.Dropout = []#intitialize list of dropout layers\n",
    "        \n",
    "        #check to see if input_dim = output_dim\n",
    "        if(len(input_features) != len(output_features)):\n",
    "            raise ValueError(\"lengths do not match. input dimension MUST equal output dimensions\")\n",
    "        \n",
    "        #check to see if dropout dim = L - 1:\n",
    "        if(len(dropout_layers) >= len(input_features) - 1):\n",
    "            raise ValueError(\"dropout layers dimensions do not match appropriate size\")\n",
    "            \n",
    "        for input_unit, output_unit in zip(input_features, output_features):\n",
    "            self.FC.append(nn.Linear(input_unit, output_unit, bias=True))#add Linear layers to the network\n",
    "        \n",
    "        #set dropout layers\n",
    "        for prob in dropout_layers:\n",
    "            self.Dropout.append(nn.Dropout(prob))#append dropout layers with keep_probs to NN.\n",
    "            \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Build a network that performs feed forward for one pass, by mapping input_space, S -> action, A.\n",
    "        Returns the corresponding model.\n",
    "        -------\n",
    "        @param:\n",
    "        1. state: (array_like) input state.\n",
    "        @Return:\n",
    "        - model: the corresponding model\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        for i in range(len(self.FC)):\n",
    "            if(i == 0):\n",
    "                X = F.relu(self.FC[0](state))\n",
    "            else:\n",
    "                X = F.relu(self.FC[i](X)) if (i < len(self.FC) - 1) else self.FC[i](X)\n",
    "            if(i < len(self.Dropout)):\n",
    "                X = self.Dropout[i](X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQNetwork(input_features, output_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
